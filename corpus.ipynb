{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Russian language dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'regex'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0f112b574aa2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfastai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmultiprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mregex\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangdetect\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdetect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangdetect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang_detect_exception\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLangDetectException\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'regex'"
     ]
    }
   ],
   "source": [
    "from fastai.basics import *\n",
    "from multiprocessing import Pool\n",
    "import regex as re\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_LINE = '<|n|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librusec = '/home/u/nas/librusec/lib.rus.ec'\n",
    "tmp = './tmp/fb2'\n",
    "data = Path('../data/full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ../data\n",
    "!mkdir ../data/full\n",
    "!mkdir ../data/classic\n",
    "!mkdir tmp\n",
    "!mkdir tmp/fb2\n",
    "!mkdir tmp/txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unpack zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zips = get_files(librusec, '.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack(fn):\n",
    "    !unzip {fn} -d {tmp} >>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in progress_bar(Pool(64).imap_unordered(unpack, zips), len(zips)):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert fb2 to txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbs = get_files(tmp, '.fb2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_fb2(fn):\n",
    "    !xsltproc FB2_2_txt.xsl {fn} > {str(fn).replace('.fb2','.txt').replace('/fb2','/txt')} 2>>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for _ in progress_bar(Pool(64).imap_unordered(convert_fb2, fbs), len(fbs)):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -R /share/CE_CACHEDEV1_DATA/data/classic_lit/*\n",
    "!cp -R /share/CE_CACHEDEV1_DATA/data/downloads/txt /share/CE_CACHEDEV1_DATA/data/classic_lit/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbs = get_files('/home/u/nas/classic_lit/txt/', '.fb2', recurse=True); len(fbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -R /home/u/nas/classic_lit/out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_out = Path('/home/u/nas/classic_lit/out'); path_out.mkdir(exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fn in fbs:\n",
    "    nn = (str(fn.name)\n",
    "        .replace(' ','.')\n",
    "        .replace('_quot;','.')\n",
    "        .replace('!','.')\n",
    "        .replace(',','.')\n",
    "        .replace('(','.')\n",
    "        .replace(')','.')\n",
    "        .replace('\\xa0','.')\n",
    "         )\n",
    "    shutil.move(fn, path_out/nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbs = get_files(path_out, '.fb2'); len(fbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm ../data/classic/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Path('../data/classic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_fb2(fn):\n",
    "    name = str(fn.name).replace('.fb2','.txt')\n",
    "    !xsltproc FB2_2_txt.xsl {fn} > {data/name} 2>>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in progress_bar(Pool(64).imap_unordered(convert_fb2, fbs), len(fbs)):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm ../data/classic/*месяцеслов*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ../data/classic/valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv ../data/classic/Tolstoy_Dva_pisma_k_M_Gandi.56185.txt ../data/classic/valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv ../data/classic/Tolstoy_Sobranie_sochineniy_v_dvadtsati_dvuh_tomah_22_Tom_22._Izbrannyie_dnevniki_1895-1910.142868.txt ../data/classic/valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv ../data/classic/Tolstoy_Sobranie_sochineniy_v_dvadtsati_dvuh_tomah_20_Tom_20._Izbrannyie_pisma_1900-1910.142866.txt ../data/classic/valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter txts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txts = get_files('./tmp/txt', '.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(txts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "fn = txts[0]\n",
    "match = re.compile('(.|\\s)\\\\1\\\\1+')\n",
    "with open(fn, 'r') as f:\n",
    "    lines = f.read()\n",
    "lines = 'asdf aaaa dddddfffff' + lines\n",
    "lines = match.sub(r'\\1'*3, lines)             \n",
    "lines[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "detect(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "expr = re.compile('([^\\n]{150,})([.] )([^\\n]{150,})')\n",
    "while expr.search(lines):\n",
    "    lines = expr.sub(r'\\g<1>.\\n\\g<3>', lines, 1)\n",
    "lines[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this will take time, bcs langdetect fails on multithreading\n",
    "for fn in progress_bar(txts):\n",
    "    with open(fn, 'r') as f:\n",
    "        lines = f.read()\n",
    "        try:\n",
    "            if len(lines) > 1e+4 and detect(lines) == 'ru':\n",
    "                with open(f'{data}/{fn.name}', 'w') as c:\n",
    "                    c.write(lines)\n",
    "        except LangDetectException as e:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txts = get_files(data, '.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txts += ['../data/poetry_base.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txts += get_files('../data/classic', '.txt', recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add space before each word. It's not really nesessary. \n",
    "# It just makes encoding a bit more meaningful to the model and the text smaller(after encoding).\n",
    "\n",
    "def process_fn(fn):\n",
    "    match = re.compile(r'(?=[^ ])([\\W])([\\w])')\n",
    "    match2 = re.compile('(.|\\s)\\\\1\\\\1+')\n",
    "    with open(fn, 'r') as f:\n",
    "        lines = f.read()\n",
    "    if lines and lines[0] != ' ': lines = ' ' + lines\n",
    "    lines = match.sub(r'\\g<1> \\g<2>', lines)\n",
    "    lines = match2.sub(r'\\1'*3, lines)\n",
    "    with open(fn, 'w') as c:\n",
    "        c.write(lines)\n",
    "        \n",
    "for _ in progress_bar(Pool(64).imap_unordered(process_fn, txts), len(txts)):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/poetry_base.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "split = int(len(lines)*0.95)\n",
    "\n",
    "with open('../data/poetry_dry.txt', 'w') as f:\n",
    "    f.writelines(lines[:split])\n",
    "\n",
    "with open('../data/poetry_eval.txt', 'w') as f:\n",
    "    f.writelines(lines[split:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### concat for vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take smallest files to increase word diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txts = get_files(data, '.txt'); len(txts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fn in txts:\n",
    "    if os.path.getsize(fn) <= 1e+3: \n",
    "        os.remove(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txts = get_files(data, '.txt'); len(txts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsorted = get_files('../data/classic', '.txt') + sorted(txts, key=lambda fn: os.path.getsize(fn))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz=0\n",
    "with open('./tmp/russian_corpus_for_vocab.txt', 'w') as c:\n",
    "    for fn in fsorted:\n",
    "        with open(fn, 'r') as f:\n",
    "            sz += c.write(f.read().replace('\\n', f' {NEW_LINE} ') + '\\n')\n",
    "            if sz > 5e+9:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cache tokenization (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_lm_finetuning import TextDataset\n",
    "from yt_encoder import YTEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txts = get_files(data, '.txt'); len(txts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_fn(fn):\n",
    "    tokenizer = YTEncoder.from_pretrained('../bpe/yt.model')\n",
    "    TextDataset.process_file(fn, tokenizer, 1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in progress_bar(Pool(32).imap_unordered(cache_fn, txts), len(txts)):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare cached dataset for upload (for GCloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_files('upload', '.txt', True); len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [item for item in files if '/full' in str(item) and '/cached' not in str(item)]; len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in files:\n",
    "    with open(item, 'w'):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
